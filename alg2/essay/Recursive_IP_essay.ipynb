{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ae68c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# # SST LR\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def concatenate(X):\n",
    "    return np.concatenate([X, np.ones((X.shape[0], 1))], axis=1)\n",
    "\n",
    "\n",
    "def error(X, y, model):\n",
    "    return model.predict_proba(X)[:, 1] - y\n",
    "\n",
    "\n",
    "def prediction(X, model):\n",
    "    return model.predict_proba(X)[:, 1]\n",
    "\n",
    "\n",
    "def gradient_(X, y, model, C):\n",
    "    F = concatenate(X)\n",
    "    #return F * error(X, y, model)[:, None] + C * w / X.shape[0]\n",
    "    return F * error(X, y, model)[:, None]\n",
    "\n",
    "\n",
    "def hessian_(X, model, C):\n",
    "    probs = prediction(X, model)\n",
    "    F = concatenate(X)\n",
    "    H = F.T @ np.diag(probs * (1 - probs)) @ F / F.shape[0] + C * np.eye(\n",
    "        F.shape[1]) / X.shape[0]\n",
    "    return H\n",
    "\n",
    "\n",
    "def inverse_hessian(H):\n",
    "    return np.linalg.inv(H)\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "def Remove(k, scores, test_idx):\n",
    "    #print(\"test_idx\", test_idx)\n",
    "    #print(\"old\")\n",
    "    #print(pred[test_idx])\n",
    "    \n",
    "    if pred[test_idx] > 0.5:\n",
    "        top_k_index = scores[test_idx].argsort()[-k:]\n",
    "    else:\n",
    "        top_k_index = scores[test_idx].argsort()[:k]\n",
    "    \n",
    "    X_r = X[\"train\"][top_k_index]\n",
    "    y_r = y[\"train\"][top_k_index]\n",
    "    X_l = np.delete(X[\"train\"], top_k_index, axis=0)\n",
    "    y_l = np.delete(y[\"train\"], top_k_index, axis=0)\n",
    "\n",
    "        \n",
    "    prediction = -np.sum(scores[test_idx][top_k_index])\n",
    "    #print(\"prediction\", prediction)\n",
    "\n",
    "    return X_l, y_l, prediction, X_r, y_r\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "def new_train(k, dev_index, scores):\n",
    "    X_k, y_k, prediction, x_r, y_r = Remove(k, scores, dev_index)\n",
    "    \n",
    "    if y_k.shape[0] == np.sum(y_k) or np.sum(y_k) == 0: # data contains only one class: 1\n",
    "        return None\n",
    "\n",
    "    # Fit the model again\n",
    "    model_k = LogisticRegression(penalty='l2', C=1)\n",
    "    model_k.fit(X_k, y_k)\n",
    "\n",
    "    # predictthe probaility with test point\n",
    "    test_point = X[\"dev\"][dev_index]\n",
    "    test_point=np.reshape(test_point, (1,-1))\n",
    "    \n",
    "    new = model_k.predict_proba(test_point)[0][1]\n",
    "    return new\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "def approximate_k(test_idx):\n",
    "    old = pred[test_idx].item()\n",
    "    for k in range(1, np.sum(y[\"train\"])):\n",
    "        X_l, y_l, pred_change_if, X_r, y_r = Remove(k, delta_pred, test_idx)\n",
    "        #_, flip,_ = new_train(k, test_idx, delta_pred)\n",
    "        \n",
    "        if old > 0.5 and pred_change_if + old < 0.5:\n",
    "            return k\n",
    "        elif old < 0.5 and pred_change_if + old > 0.5:\n",
    "            return k\n",
    "        \n",
    "    return None\n",
    "\n",
    "\n",
    "# In[60]:\n",
    "\n",
    "\n",
    "def recursive_NT(test_idx, old, X_0, y_0, model, l2, I=100, D=1):\n",
    "    eps = 1 / X[\"train\"].shape[0]\n",
    "    X_l = X_0\n",
    "    y_l = y_0\n",
    "    X_r = X_0\n",
    "    y_r = y_0\n",
    "    # Compute IP\n",
    "    F_dev = np.concatenate([X[\"dev\"], np.ones((X[\"dev\"].shape[0], 1))], axis=1)\n",
    "    new_hessain = hessian_(X_l, model, l2)\n",
    "    new_inv = inverse_hessian(new_hessain)\n",
    "    new_grad_train = gradient_(X_r, y_r, model, l2)\n",
    "    delta_k = -eps * new_inv @ new_grad_train.T\n",
    "    grad_f = F_dev[test_idx] * (old * (1 - old))\n",
    "    delta_pred = grad_f @ delta_k\n",
    "\n",
    "    K_new = y_0.shape[0]\n",
    "    predicted_change_new = None\n",
    "    ite = 0\n",
    "    diff = K_new - 0\n",
    "    removed_order = []\n",
    "    while (ite < I and diff > D and K_new != 1):\n",
    "        ite += 1\n",
    "\n",
    "        if old > 0.5:\n",
    "            sorted_index = np.flip(delta_pred.argsort())\n",
    "        else:\n",
    "            sorted_index = delta_pred.argsort()\n",
    "\n",
    "        for k in range(1, y_r.shape[0]):\n",
    "            top_k_index = sorted_index[:k]\n",
    "            predicted_change = -np.sum(delta_pred[top_k_index])\n",
    "            \"\"\"\n",
    "            print(\"K\", k)\n",
    "            print(\"predicted_change\", predicted_change)\n",
    "            print(\"old\", np.round(old))\n",
    "            print(\"K\", k)\n",
    "            print(\"removed shape\", X_r.shape)\n",
    "            print(\"left shape\", X_l.shape)\n",
    "            print(\"delta_pred shape\", delta_pred.shape)\n",
    "            print(\"predicted_change\", predicted_change)\n",
    "            print(\"old\", np.round(old), \"new\", np.round(old + predicted_change))\n",
    "            print()\n",
    "            \"\"\"\n",
    "\n",
    "            if np.round(old) != np.round(old + predicted_change):\n",
    "                # print(\"K\", k)\n",
    "\n",
    "                diff = K_new - k\n",
    "                K_new = k\n",
    "                predicted_change_new = predicted_change\n",
    "\n",
    "                index_whole = []\n",
    "                for idx_r in top_k_index:\n",
    "                    point = X_r[idx_r]\n",
    "                    idx_0 = X_dist[str(point.tolist())]\n",
    "                    index_whole.append(idx_0)\n",
    "\n",
    "                X_r = X_r[top_k_index]\n",
    "                y_r = y_r[top_k_index]\n",
    "\n",
    "                X_l = np.delete(X_0, index_whole, axis=0)\n",
    "                y_l = np.delete(y_0, index_whole, axis=0)\n",
    "                # print(\"removed shape\", X_r.shape)\n",
    "                # print(\"left shape\", X_l.shape)\n",
    "\n",
    "                # new hessian for the left points\n",
    "                new_hessain = hessian_(X_l, model, C=l2)\n",
    "                new_inv = inverse_hessian(new_hessain)\n",
    "                # new gradient for the removed points\n",
    "                new_grad_train = gradient_(X_r, y_r, model, C=l2)\n",
    "\n",
    "                delta_k = -eps * new_inv @ new_grad_train.T\n",
    "                grad_f = F_dev[test_idx] * (old * (1 - old))\n",
    "                delta_pred = grad_f @ delta_k\n",
    "                break\n",
    "\n",
    "            if k == y_r.shape[0] - 1:\n",
    "                if K_new == y_0.shape[0] and diff == y_0.shape[0]:\n",
    "                    return None, predicted_change_new, ite, None, []\n",
    "                \n",
    "                return K_new, predicted_change_new, ite, diff, index_whole\n",
    "\n",
    "    return K_new, predicted_change_new, ite, diff, index_whole\n",
    "\n",
    "\n",
    "# In[6]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e96a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# In[23]:\n",
    "\n",
    "\n",
    "X = {}\n",
    "y = {}\n",
    "X[\"train\"] = np.load(\"X_train_essay_LR.npy\")\n",
    "y[\"train\"] = np.load(\"y_train_essay_LR.npy\")\n",
    "X[\"dev\"] = np.load(\"X_dev_essay_LR.npy\")\n",
    "y[\"dev\"] = np.load(\"y_dev_essay_LR.npy\")\n",
    "\n",
    "\n",
    "# In[24]:\n",
    "\n",
    "\n",
    "X[\"train\"].shape, X[\"dev\"].shape, y[\"train\"].shape, y[\"dev\"].shape\n",
    "\n",
    "\n",
    "# In[25]:\n",
    "\n",
    "\n",
    "l2 = 1\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(penalty='l2', C=1)\n",
    "model.fit(X[\"train\"], y[\"train\"])\n",
    "model.score(X[\"dev\"], y[\"dev\"])\n",
    "w = np.concatenate((model.coef_, model.intercept_[None, :]), axis=1)\n",
    "F_train = np.concatenate([X[\"train\"], np.ones((X[\"train\"].shape[0], 1))], axis=1) # Concatenating one to calculate the gradient with respect to intercept\n",
    "F_dev = np.concatenate([X[\"dev\"], np.ones((X[\"dev\"].shape[0], 1))], axis=1)\n",
    "\n",
    "error_train = model.predict_proba(X[\"train\"])[:, 1] - y[\"train\"]\n",
    "error_dev = model.predict_proba(X[\"dev\"])[:, 1] - y[\"dev\"]\n",
    "\n",
    "gradient_train = F_train * error_train[:, None]\n",
    "gradient_dev = F_dev * error_dev[:, None]\n",
    "\n",
    "#from scipy import sparse\n",
    "probs = model.predict_proba(X[\"train\"])[:, 1]\n",
    "H = F_train.T @ np.diag(probs * (1 - probs)) @ F_train / X[\"train\"].shape[0] + 1 * np.eye(F_train.shape[1]) / X[\"train\"].shape[0]\n",
    "inverse_H = np.linalg.inv(H)\n",
    "\n",
    "eps = 1 / X[\"train\"].shape[0]\n",
    "delta_k = -eps * inverse_H @ gradient_train.T\n",
    "pred = np.reshape(model.predict_proba(X[\"dev\"])[:, 1], (model.predict_proba(X[\"dev\"])[:, 1].shape[0], 1))\n",
    "grad_f = F_dev * (pred * (1 - pred))\n",
    "delta_pred = grad_f @ delta_k\n",
    "delta_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ceb93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dist = {}\n",
    "for i in range(X[\"train\"].shape[0]):\n",
    "    X_dist[str(X[\"train\"][i].tolist())] = i\n",
    "\n",
    "NT_app_k = []\n",
    "new_predictions = []\n",
    "iterations = []\n",
    "diffs = []\n",
    "order_lists = []\n",
    "\n",
    "for i in range(X[\"dev\"].shape[0]):\n",
    "    test_idx = i\n",
    "    print(\"test_idx\", test_idx)\n",
    "\n",
    "    old = pred[test_idx].item()\n",
    "    print(\"old\", old)\n",
    "\n",
    "    K_nt, pred_change_nt, ite, diff, order= recursive_NT(test_idx, old, X[\"train\"], y[\"train\"], model, l2, I=100, D=1)\n",
    "\n",
    "    if pred_change_nt != None:  \n",
    "        new_nt = new_train(K_nt, test_idx, delta_pred)\n",
    "    else:\n",
    "        new_nt = None\n",
    "\n",
    "            \n",
    "    print(\"K_nt, pred_change_nt, ite, diff\")\n",
    "    print(K_nt, pred_change_nt, ite, diff)\n",
    "    print(\"new\", new_nt)\n",
    "    print()\n",
    "    NT_app_k.append(K_nt)\n",
    "    new_predictions.append(new_nt)\n",
    "    iterations.append(ite)\n",
    "    diffs.append(diff)\n",
    "    order_lists.append(order)\n",
    "\n",
    "np.save(\"NT_app_k_essay_LR_I100_D1.npy\", NT_app_k)\n",
    "np.save(\"new_predictions_essay_LR_I100_D1.npy\", new_predictions)\n",
    "np.save(\"iterations_essay_LR_I100_D1.npy\", iterations)\n",
    "np.save(\"diffs_essay_LR_I100_D1.npy\", diffs)\n",
    "np.save(\"order_essay_LR_I100_D1.npy\", order_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d252d41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = {}\n",
    "y = {}\n",
    "X[\"train\"] = np.load(\"train_feature_essay.npy\")\n",
    "y[\"train\"] = np.load(\"train_label_essay.npy\").squeeze()\n",
    "X[\"dev\"] = np.load(\"test_feature_essay.npy\")\n",
    "y[\"dev\"] = np.load(\"test_label_essay.npy\").squeeze()\n",
    "X[\"train\"].shape, X[\"dev\"].shape, y[\"train\"].shape, y[\"dev\"].shape\n",
    "\n",
    "\n",
    "# In[20]:\n",
    "\n",
    "\n",
    "l2 = 10\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(penalty='l2', C=0.1)\n",
    "model.fit(X[\"train\"], y[\"train\"])\n",
    "model.score(X[\"dev\"], y[\"dev\"])\n",
    "w = np.concatenate((model.coef_, model.intercept_[None, :]), axis=1)\n",
    "F_train = np.concatenate([X[\"train\"], np.ones((X[\"train\"].shape[0], 1))], axis=1) # Concatenating one to calculate the gradient with respect to intercept\n",
    "F_dev = np.concatenate([X[\"dev\"], np.ones((X[\"dev\"].shape[0], 1))], axis=1)\n",
    "\n",
    "error_train = model.predict_proba(X[\"train\"])[:, 1] - y[\"train\"]\n",
    "error_dev = model.predict_proba(X[\"dev\"])[:, 1] - y[\"dev\"]\n",
    "\n",
    "gradient_train = F_train * error_train[:, None]\n",
    "gradient_dev = F_dev * error_dev[:, None]\n",
    "\n",
    "#from scipy import sparse\n",
    "probs = model.predict_proba(X[\"train\"])[:, 1]\n",
    "H = F_train.T @ np.diag(probs * (1 - probs)) @ F_train / X[\"train\"].shape[0] + l2 * np.eye(F_train.shape[1]) / X[\"train\"].shape[0]\n",
    "inverse_H = np.linalg.inv(H)\n",
    "eps = 1 / X[\"train\"].shape[0]\n",
    "delta_k = -eps * inverse_H @ gradient_train.T\n",
    "pred = np.reshape(model.predict_proba(X[\"dev\"])[:, 1], (model.predict_proba(X[\"dev\"])[:, 1].shape[0], 1))\n",
    "grad_f = F_dev * (pred * (1 - pred))\n",
    "delta_pred = grad_f @ delta_k\n",
    "delta_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7b3f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dist = {}\n",
    "for i in range(X[\"train\"].shape[0]):\n",
    "    X_dist[str(X[\"train\"][i].tolist())] = i\n",
    "    \n",
    "NT_app_k = []\n",
    "\n",
    "new_predictions = []\n",
    "iterations = []\n",
    "diffs = []\n",
    "order_lists = [] \n",
    "\n",
    "def new_train(k, dev_index, scores):\n",
    "    X_k, y_k, prediction, x_r, y_r = Remove(k, scores, dev_index)\n",
    "    \n",
    "    if y_k.shape[0] == np.sum(y_k) or np.sum(y_k) == 0: # data contains only one class: 1\n",
    "        return None\n",
    "\n",
    "    # Fit the model again\n",
    "    model_k = LogisticRegression(penalty='l2', C=0.1)\n",
    "    model_k.fit(X_k, y_k)\n",
    "\n",
    "    # predictthe probaility with test point\n",
    "    test_point = X[\"dev\"][dev_index]\n",
    "    test_point=np.reshape(test_point, (1,-1))\n",
    "    \n",
    "    new = model_k.predict_proba(test_point)[0][1]\n",
    "    return new\n",
    "\n",
    "for i in range(X[\"dev\"].shape[0]):\n",
    "    test_idx = i\n",
    "    print(\"test_idx\", test_idx)\n",
    "\n",
    "    old = pred[test_idx].item()\n",
    "    print(\"old\", old)\n",
    "\n",
    "    K_nt, pred_change_nt, ite, diff, order= recursive_NT(test_idx, old, X[\"train\"], y[\"train\"], model, l2, I=100, D=1)\n",
    "\n",
    "    if pred_change_nt != None:  \n",
    "        new_nt = new_train(K_nt, test_idx, delta_pred)\n",
    "    else:\n",
    "        new_nt = None\n",
    "\n",
    "        \n",
    "    print(\"K_nt, pred_change_nt, ite, diff\")\n",
    "    print(K_nt, pred_change_nt, ite, diff)\n",
    "    print(\"new\", new_nt)\n",
    "    print()\n",
    "    NT_app_k.append(K_nt)\n",
    "    new_predictions.append(new_nt)\n",
    "    iterations.append(ite)\n",
    "    diffs.append(diff)\n",
    "    order_lists.append(order)\n",
    "\n",
    "np.save(\"NT_app_k_essay_bertT_I100_D1.npy\", NT_app_k)\n",
    "np.save(\"new_predictions_essay_bertT_I100_D1.npy\", new_predictions)\n",
    "np.save(\"iterations_essay_bertT_I100_D1.npy\", iterations)\n",
    "np.save(\"diffs_essay_bertT_I100_D1.npy\", diffs)\n",
    "np.save(\"order_essay_bertT_I100_D1.npy\", order_lists)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
